---
title: "Linear Models and Generalized Linear Models as Descriptive Tools"
author: "Pavel Logacev"
date: "October 14, 2018"
output: 
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Statistical Models
- Phenomenological models
- Mechanistic models / Process models
- Statistical Models vs. Functional/Deterministic Models



# Deterministic Linear Models 

In this section we are going to explore **deterministic** linear models, i.e. linear models that work on **idealized** data, when there is no measurement error. The basic form of a linear regression model, in a form that works for *idealized* data:
<!-- 
- What is regression?
- What is linear about this model?
-->

$$ \underbrace{y}_{\text{Observed value}} =
            \overbrace{\underbrace{\alpha}_{\text{Intercept}}}^{\text{additive term}} + 
            \overbrace{\underbrace{\beta_1}_{\text{Slope}} * \underbrace{x_1}_{\text{Predictor}}}^{\text{additive term}} + 
            \overbrace{\underbrace{\beta_2}_{\text{Slope}} * \underbrace{x_2}_{\text{Predictor}}}^{\text{additive term}} +  
            \ldots$$
<!--  \overbrace{\ldots}^{\text{more additive terms}} -->
            
Two types are usually distinguished: *single-variable*, and *multi-variable* (also: *simple linear regression*, *multiple linear regression*).
<!-- 'univariate', and 'multivariate' stand for the number of dependent variables, *not* the number of predictors  -->

Throughout this course, we will make use of the (Generalized) Linear Model as the main statistical model because it is:

- fairly easy to use

- suffficiently versatile for most practical purposes

- well-studied




```{r echo=FALSE, results='hide', message=FALSE}
library(tidyverse)
library("scatterplot3d")
library(magrittr)
library(languageR)
library(ggplot2)
theme_set(theme_bw())

labelled_arrow <- function(p, x_start, y_start, x_end, y_end, x_label, y_label, label, color, arrowhead_size = unit(0.2, "cm")) {
    p + geom_segment(data=NULL, aes(x=x_start, y=y_start, xend=x_end, yend=y_end), 
                        color = color, arrow = arrow(length = arrowhead_size), size = 1) + 
          geom_text(x = x_label, y = y_label, label = label, color = color)
}

labelled_slope_arrow <- function(p, x_start, y_start, x_end, y_end, 
                                 x_label, y_label, label, color, 
                                 arrowhead_size = unit(0.2, "cm"))
{
    p <- p + geom_segment(data=NULL, aes(x=x_start, y=y_start, xend=x_end, yend=y_start), color = color, size = 1)
    p %>% labelled_arrow(x_start = x_end, y_start = y_start, x_end = x_end, y_end = y_end, 
                         x_label = x_label, y_label = y_label, label = label, color = color)
}

cab_intercept <- 4.0
cab_slope_distance <- 2.5
cab_slope_nbridges <- 5
cab_slope_color_red <- 30
cab_bridge <- 10

cab_fares <- data.frame( distance_km = c(3,7,9,11,12,13,12,16,18) ) # ,22,27
cab_fares$fare_mnt <- cab_intercept + cab_slope_distance * cab_fares$distance_km

cars_perfect <- data.frame(speed = 1, weight = 1, distx = 1, dist = 1)

```


## Single-variable Model

Let's take a look at a dataset of taxi rides in an unknown city. Imagine that I have taken `r nrow(cab_fares)` taxi rides, and recorded the travel distance, as well as the taxi fare for reach ride. 
The following plot shows each of these measurements as a point with the distances (in km) on the x-axis, and the fare (in Mongolian tögrög; MNT) on the y-axis. This dataset is interesting, because you probably understand the typical relationships between ride distance and fare fairly well, so it is an ideal example for illustrating linear models.

```{r carsPerfect1, fig.height=2.5, echo=FALSE}
cab_fares %>% ggplot(aes(distance_km, fare_mnt)) + geom_point() + xlab("distance (km)") + ylab("fare (MNT)")
```

Unsurprisingly, we can see clearly that the fare increases with distance. In other words, *distance* and *fare* have a positive relationship (if one increases, so does the other). 
The relationship between the two variables is so strong that if we know the value of one of the two variables for a particular data point, we can predict the value of the other with a high degree of confidence. (You might want to say that I can predict if with *absolute certainty*, but that presupposes precise knowledge of the cab fare system in that city, *and* an extreme degree of trust towards the local taxi drivers. Let's assume that we don't have either.)

We can make that prediction with such a high degree of certainty because all the points seem to lie on a line. If we learned the function that describes this line, we could learn much more from this dataset than the simple fact that the two variables a positive correlated. That is unsurprising to begin with, but we may want to know **how exactly** they are related. 

A simple way to capture this relationship is to try and describe it with this function:

$$ \text{fare} = a + b * \text{distance} $$

What this equation says is that if we take a value for distance, multiply it by *some number* called $b$, and then add *another number* called $a$, we will know the fare that corresponds to this distance. (In other words, this equation posits that the relationship between *distance* and *fare* is - at least approximately - linear.[^1]) In this particular case the relationship is $\text{fare} = 4 + 2.5 * \text{distance}$, which means that $a=4$, and $b=2.5$. We can verify that this equation is indeed the correct generalization by visualizing this function as a line in the plot below: As you can see, it accurately describes all the points in the graph. 

[^1]: Please note that we don't neeed to assume that the *'true relationship'* between the two variables is actually linear. Most interesting relationships are not linear. However, we can still learn a lot about them from a linear model as you will see in the following.
<!-- to-do: In the following, sprinkle in sections about what we can still learn even if the relationship is not actually linear. -->

```{r carsPerfect2, fig.height=2.5, echo=FALSE}
cab_fares %>% ggplot(aes(distance_km, fare_mnt)) + geom_point() + xlab("distance (km)") + ylab("fare (MNT)") + geom_abline(intercept = 4, slope = 2.5, color = "red")
```

Importantly, if we conceptualize $a$ and $b$ as just some numbers that we need to add and multiply by, we will miss an important insight: Both numbers have useful interpretations. In this case, $a$ and $b$ can be interpreted as follows: 

- $a$ (called the **intercept**) can be interpreted as the value of *fare* when *distance* is 0. In this case, the intercept is $4~MNT$, which is the amount you have to pay if you get in and change your mind after the taxometer has been switched on (if the taxi driver is a real stickler for rules).

- $b$ (called the **slope** for **distance**) can be interpreted as the additional amount of money you have to pay for every additional kilometer travelled. A slope of $2.5$ means that a distance increase of $1~km$ increases the stopping distance by $2.5~MNT$.   

The plot below illustrates these interpretations:

```{r, fig.height=3.5, echo=FALSE}
  a = 4; b1 = 2.5;

  p <- cab_fares %>% ggplot(aes(distance_km, fare_mnt)) + geom_point()
  p <- p + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) + 
            geom_abline(intercept = a, slope = b1, color = "red")
  
  p <- p %>% labelled_arrow(x_start=0, y_start=0, x_end=0, y_end=a,
                            x_label = 1.5, y_label = 2, 
                            label = "Intercept", color = "blue")

  p %>% labelled_slope_arrow(x_start=0, y_start=a, x_end=1, y_end=a+b1, 
                             x_label = 2, y_label = a+b1/2, label = "Slope", color = "green")

```

<!-- TODO: Take-away message. -->


## Multi-variable Model

```{r }

n = 100 
set.seed(123)
cab_fares2 <- data.frame(
        distance_km = runif(n, min=3, max=27) %>% round(),
        n_bridges = runif(n, min=1, max=10) %>% round()
              )
cab_fares2$fare_mnt <- with(cab_fares2, cab_intercept + cab_slope_distance*distance_km + 
cab_slope_nbridges*n_bridges)

cab_fares$n_bridges <- 1
cab_fares2 %<>% bind_rows(cab_fares)

```

Let's imagine that this imaginary city has many bridges. Since I wasn't sure whether bridge tolls apply, I also recorded the number of bridges crossed on each trip. In the previous section, we've looked at the subset of data where no bridges were crossed. Let's see how the fare depends not only on distance travelled, but also on the number of bridges crossed.

Now, we are looking at the relationship between three variables, and the data frame looks as follows:

```{r }
DT::datatable(cab_fares2, options = list(searching = FALSE))
```

We will again assume that the relationship between fare, distance and number of bridges is linear, and that they contribute to the fare independently. We can now describe the relationship with the following equation:
$$ fare = a + b_1 * distance + b_2 * N_{bridges} $$
The correct parameter values for this equation are $a=4$, $b_1 = 2.5$ (as previously), and $b_2=5$. The interpretation of the coefficients is similar to the previous section:

- $a$ (the **intercept**) is the fare when no bridges have been crossed, and the travel distance is zero.[^2] 

- $b_1$ (the **slope** for **distance**) is the additional fare for every additional kilometer.

- $b_2$ (the **slope** for **$N_{bridges}$**) is the additional fare for every additional bridge.

[^2]: While the number of bridges depends on distance, it is in principle possible to cross a bridge while keeping the travel distance below a kilometer, in which case it would count as zero.

<!-- Consider 
- integrating how the intercept wouldn't make any sense with number of passengers. 
- use day/night for an interaction. 
-->

The following plot shows the data vis-à-vis the linear model fits. Because we are dealing with a relationship between three variables, every datapoint is a point in three-dimensional space (x-axis: distance, y-axis: number of bridges, z-axis: fare). The multi-variable model fit is illustrated by the black plane which rises with increasing distance and/or number of bridges crossed.

If you enable the display of the single-variable model from the last section, you'll see that it also describes a plane: However, the plane corresponding to the single-variable model does not rise with the number of bridges crossed. This is because it is not used as a predictor in that model. 

You will also notice that the red plane and the black plane intersect in a line at $N_{bridges} = 0$. This is because the two model equations $a + b_1 * distance$ and  $a + b_1 * distance + b_2 * N_{bridges}$ are equivalent for $N_{bridges} = 0$. 

```{r, fig.height=2, fig.width=2, fig.align='right'}
inputPanel(
  checkboxInput("modelOn", label = "Show multi-variable model:", value = TRUE),
  checkboxInput("originalModelOn", label = "Show old single-variable model:", value = FALSE),
  sliderInput("angle", label = "Plot Angle", min = 0, max = 360, value = 125)
)
```

```{r, fig.height=2, fig.width=2, fig.align='right'}

renderPlot({
  a = cab_intercept; b1 = cab_slope_distance; b2 = cab_slope_nbridges;
  
  full_lm <- lm(fare_mnt ~ distance_km + n_bridges, cab_fares2)
  summary(full_lm)

  angle <- 125
  if (!is.null(input)) angle <- input$angle
  s3d <- scatterplot3d(cab_fares2, type = "h", color = "blue", angle=angle, pch = 16) # input$value
  #s3d$plane3d(Intercept = -48, x.coef = 0, y.coef = 5, col = "blue")
  
  if (as.logical(input$modelOn)) {
    s3d$plane3d(Intercept = 4, x.coef = 2.5, y.coef = 5)
  }
  if (as.logical(input$originalModelOn)) {
    s3d$plane3d(Intercept = 10, x.coef = 2.5, y.coef = 0, col = "red")
  }
  s3d
})
```


## Categorical Predictors

Predictors on the **nominal** or **ordinal scale** (as opposed to **ratio**, or **interval scale**) are not naturally represented by numbers. 
For example: Speaker gender, Presence or absence of wh-movement, word order (e.g., SOV, SVO, VSO), size (long, short).
We can represent them numerically, but there are many coding options. We can represent a factor with two levels by 0 and 1, or by -1 and 1, or by +0.5 and -0.5. 

In the case of the taxi fares, one such predictor is the color of the taxi: I've observed red and blue taxis, and I didn't know if there is possibly a difference in their pricing. In order to determine whether there is, I decided to take a ride in both types. (All the data in the previous sections was for yellow cabs only.)

The following plot shows the relationship between fare and cab color (for $distance=10~km$, and $N_{bridges} = 0$). The x-axis shows the **contrast** for cab color (*the numerical representation of of the categorical variable 'color'*). The y-axis shows the cab fare. 

While the choice of the coding scheme is somwewhat arbitraty, it will affect the interpretation of the coefficients.
<!-- Make sure whe word 'coefficients' is known at this point, ideally by moving the deterministic form of the LM forward. -->
In other words, **model parameterization** (or: **contrast specification**) affects the meaning of the coefficients. 
We will use the following equation to describe the relationship:


$$ \text{fare} = a + b * \text{cCabColor} $$, where $cCabColor$ is a numerical representation of the cab color. 

We can use **treatment contrasts** (yellow = 0, red = 1) or **sum contrasts** (yellow = -0.5, red = 0.5). 

When we use **treatment contrasts**, the coefficients receive the following interpretation:

- $a$: the fare ride in a yellow taxi (for a $10 km$ ride, when no bridges have been crossed).

- $b$: the fare difference between a ride in a red taxi and a yellow taxi (for a $10 km$ ride, when no bridges have been crossed).

When we use **sum contrasts**, the coefficients receive the following interpretation:

- $a$ the average of the fares for a ride in a yellow taxi and one in a red taxi (for a $10 km$ ride, when no bridges have been crossed).[^3] 

- $b$ the fare difference between a ride in a red taxi and a yellow taxi (for a $10 km$ ride, when no bridges have been crossed).

[^3]: Please note that it is the average of the two fares, and **not** the average fare. <!-- Explain how they are different -->



```{r }

n = 30 
cab_fares3 <- data.frame(
        distance_km = 10,
        n_bridges = 0,
        cab_color_red = c(0, 1)
        )
cab_fares3$fare_mnt <- with(cab_fares3, cab_intercept + cab_slope_distance*distance_km + 
cab_slope_nbridges*n_bridges + 
cab_slope_color_red*cab_color_red)

```

```{r}
inputPanel(
   selectInput("contrastCat1", "Contrast:",
              c("Treatment Contrast" = "treat",
                "Sum Contrast" = "sum")),
  checkboxInput("lineOnCat1", label = "Show Model:", value = FALSE),
  checkboxInput("coefsOnCat1", label = "Show Coefficients:", value = FALSE)
)  

```

```{r, fig.height=2, fig.width=2, fig.align='right'}

renderPlot({

  if (exists("input") && input$contrastCat1 == "sum") {
    cab_fares3$cCab_color_red <- cab_fares3$cab_color_red - 0.5 
  } else {
    cab_fares3$cCab_color_red <- cab_fares3$cab_color_red
  }
  
  coefs <- coef(lm(fare_mnt ~ cCab_color_red, cab_fares3))
  a = coefs[1]; b1 = coefs[2];


  p <- cab_fares3 %>% ggplot(aes(cCab_color_red, fare_mnt)) + geom_point() + xlab("cab color contrast") + ylab("fare (MNT)")
  
  if (as.logical(input$lineOnCat1)) {
    p <- p + geom_abline(intercept = a, slope = b1, color = "red")
  }
  if (!as.logical(input$coefsOnCat1)) {
    #p <- p  + scale_x_continuous(limits = c(15, 21)) +
    #          scale_y_continuous(limits = c(30, 55))
  } else {
    #p <- p  + scale_x_continuous(limits = c(0, 1)) +
    #          scale_y_continuous(limits = c(0, 1))
    p <- p + geom_hline(yintercept = 0) + geom_vline(xintercept = 0)
    
    p <- p %>% labelled_arrow(x_start = 0, y_start = 0,
                              x_end = 0, y_end = a, 
                              x_label = .1, y_label = 8, 
                              label = "Intercept", color = "blue")
    
    p <- p %>% labelled_slope_arrow(x_start = 0, y_start = a,
                                    x_end = 1, y_end = a + b1,
                                    x_label = .9, y_label = 40,
                                    label = "Slope", color = "green")
      
  }
  p + ggtitle(sprintf("%0.2f + %0.2f*x", a, b1))
})
```




## Centering predictors and its effect on intercepts

Now let's imagine that I don't know if the number of passengers affects the fare. So far I've been riding alone. Now let's look at a few rides with several other people ($distance = 10~km$, $N_{bridges} = 0$, yellow cabs only). 

```{r }

n = 30
cab_slope_npassengers = 5
  
cab_fares4 <- data.frame(
        distance_km = 10,
        n_bridges = 0,
        cab_color_red = 0,
        n_passengers = runif(n, min = 1, max = 4) %>% round()
        )

cab_fares4$fare_mnt <- with(cab_fares4, cab_intercept + 
                                        cab_slope_distance * distance_km + 
                                        cab_slope_nbridges * n_bridges + 
                                        cab_slope_color_red * cab_color_red +
                                        cab_slope_npassengers * (n_passengers-1)
                                        )

```


As previously, we will model the relationship with a linear equation:

$$ \text{fare} = a + b * \text{number of passengers} $$

However if we model it like this, our intercept is `r cab_intercept + 10*cab_slope_distance - 1*cab_slope_npassengers` as you see in the plot below, and not `r cab_intercept + 10*cab_slope_distance`, as we would expect. We did travel 10 km, after all. - Why is the fare so low?

The reason is that the intercept is the value of the fare when **all** predictors are 0 - yes, even the number of passengers. Barring any extremely unusual situations, this is not actually possible, and therefore an intercept of this kind is really just a number we need to plug into the equation. In other words, we don't learn anything from it.

In order to remedy the situation and obtain a more useful intercept, we can **center** the predictor - that is, subtract the average of the vector from the vector itself. In this case, we will use

$$ \text{fare} = a + b * (\text{number of passengers}-2.5) $$


This will leave us with the following interpretations of the coefficients:

- $a$: the fare paid for the average number of passengers, in this case for 2.5 passengers (on a 10 km ride, crossing 0 bridges, in a yellow cab)

- $b$: the additional fare for every additional passenger (on a 10 km ride, crossing 0 bridges, in a yellow cab)


```{r}
inputPanel(
  checkboxInput("centerPassengers", label = "Center passengers:", value = FALSE),
  checkboxInput("lineOnPassengers", label = "Show Model:", value = FALSE),
  checkboxInput("coefsOnPassengers", label = "Show Coefficients:", value = FALSE)
)  
```

```{r, fig.height=2, fig.width=2, fig.align='right'}

renderPlot({

  if (exists("input") && input$centerPassengers) {
    cab_fares4$cn_passengers <- cab_fares4$n_passengers - 2.5
  } else {
    cab_fares4$cn_passengers <- cab_fares4$n_passengers
  }
  
  coefs <- coef(lm(fare_mnt ~ cn_passengers, cab_fares4))
  a = coefs[1]; b1 = coefs[2];


  p <- cab_fares4 %>% ggplot(aes(cn_passengers, fare_mnt)) + geom_point() + xlab("number of passengers") + ylab("fare (MNT)")
  
  if (as.logical(input$lineOnPassengers)) {
    p <- p + geom_abline(intercept = a, slope = b1, color = "red")
  }
  if (!as.logical(input$coefsOnPassengers)) {
    #p <- p  + scale_x_continuous(limits = c(15, 21)) +
    #          scale_y_continuous(limits = c(30, 55))
  } else {
    #p <- p  + scale_x_continuous(limits = c(0, 1)) +
    #          scale_y_continuous(limits = c(0, 1))
    p <- p + geom_hline(yintercept = 0) + geom_vline(xintercept = 0)
    
    p <- p %>% labelled_arrow(x_start = 0, y_start = 0,
                              x_end = 0, y_end = a, 
                              x_label = .5, y_label = 8, 
                              label = "Intercept", color = "blue")
    
    p <- p %>% labelled_slope_arrow(x_start = 0, y_start = a,
                                    x_end = 1, y_end = a + b1,
                                    x_label = 1.25, y_label = 28,
                                    label = "Slope", color = "green")
      
  }
  p + ggtitle(sprintf("%0.2f + %0.2f*x", a, b1))
})
```




## Main Effects and Intreractions

So far, we've assumed that the effects of all variables are strictly additive. What if they are not?
Previously, we've only looked at taxi rides during the day. What about night fares? 
(We're looking at $$N_{bridges} = 0$, yellow cabs only, one passenger). 

We could model the relationship with the following equation, but in doing so we would neglect that night fares might not be an additive constant, such as $b$ in the equation below, but might actually affect the price paid per kilometer.
$$ \text{fare} = a + b * cNight$$, where $cNight$ is the contast for whether or not we're riding at night time.

It turns out that we can account for that too:

$$ \text{fare} = a + b_1 * distance + b_2 * cNight + b_3 * distance * cNight $$




```{r, eval = FALSE }

n = 30
cab_slope_night = 10
cab_slope_nightByDistance = 1
  
cab_fares5 <- data.frame(
        distance_km = 10,
        n_bridges = 0,
        cab_color_red = 0,
        n_passengers = 1,
        is_night = c(0,1)
        )

cab_fares5$fare_mnt <- with(cab_fares5, cab_intercept + 
                                        cab_slope_distance * distance_km + 
                                        cab_slope_nbridges * n_bridges + 
                                        cab_slope_color_red * cab_color_red +
                                        cab_slope_npassengers * (n_passengers-1) +
                              
                                        cab_slope_night * is_night +
                                        cab_slope_nightByDistance * is_night * distance
                                        )

```

<!-- Go through a calculation in a little table, to see how the process works with non-centered and with centered predictors -->

<!-- In all these sections, I never actually tell how I arrive at the model estimates. I should probably mention this in a few words somewhere at the beginning of the section. -->

## Effects of Omitted Variables

So far, we have been working with specific subsets of the data which kept all variables that were irrelevant for our present purposes constant (e.g., $distance=10~km$, $N_{bridges} = 0$). We were able to do that because we knew which variables are relevant since the dataset was created artificially. In real-life scenarios this is not always possible - not even when describing a deterministic system such as the cab fare pricing scheme. This is because (i) we often don't know all the relevant variables, (ii) we don't know in which ways they may interact, and (iii) even if we knew all that, we may be left with very little data if we keep taking subsets with specific characteristics all the time. (For example, a dataset with $N_{bridges} = 0$ may not have a lot of rides with $distance > 1~km$ if the city has many bridges, as I said it did.)   
<!-- Include this?: Moreover, some variables may not have been recorded. --> 

The bottom line is that in any realistic situation, we will omit relevant predictors from the model specification. Let's look at how this would affect our analysis by applying a single-variable model to the data from section 2.2, which varies in distance an number of bridges.
We will use a the single-variable model

$$ fare = a + b * distance$$


The plot below is not as clean as the previous plots - and it doesn't seem to be possible to perfectly describe all the points by one line. While the average fare seems to increase with distance, there is also quite a lot of variability. This is because the number of bridge crossings is neglected here. It is as if we were looking at a photo of a three-dimensional plot from the side.


We can still fit our linear model, but with one important difference: For the previous models, the coefficients were chosen sucht that the fare was predicted with absolute precision. Our system was deterministic since we knew every every variable that mattered and how it was to be included into the linear model equation. Once we neglect some variables, the system becomes non-deterministic - we can't predict the fare exactly, since we don't use the information about bridges.  


```{r}
inputPanel(
  checkboxInput("centerOmitted", label = "Center distance:", value = FALSE),
  checkboxInput("lineOnOmitted", label = "Show Model:", value = FALSE),
  checkboxInput("coefsOnOmitted", label = "Show Coefficients:", value = FALSE)
)  
```

```{r, fig.height=2, fig.width=2, fig.align='right'}

renderPlot({

  if (exists("input") && input$centerOmitted) {
    cab_fares2$tDistance  <- cab_fares2$distance - mean(range(cab_fares2$distance))
  } else {
    cab_fares2$tDistance <- cab_fares2$distance
  }
  
  coefs <- coef(lm(fare_mnt ~ tDistance, cab_fares2))
  a = coefs[1]; b1 = coefs[2];

  p <- cab_fares2 %>% ggplot(aes(tDistance, fare_mnt)) + geom_point() + xlab("distance") + ylab("fare (MNT)")
  
  if (as.logical(input$lineOnOmitted)) {
    p <- p + geom_abline(intercept = a, slope = b1, color = "red")
  }
  if (!as.logical(input$coefsOnOmitted)) {
    #p <- p  + scale_x_continuous(limits = c(15, 21)) +
    #          scale_y_continuous(limits = c(30, 55))
  } else {
    #p <- p  + scale_x_continuous(limits = c(0, 1)) +
    #          scale_y_continuous(limits = c(0, 1))
    p <- p + geom_hline(yintercept = 0) + geom_vline(xintercept = 0)
    
    p <- p %>% labelled_arrow(x_start = 0, y_start = 0,
                              x_end = 0, y_end = a, 
                              x_label = 3, y_label = 8, 
                              label = "Intercept", color = "blue")
    
    p <- p %>% labelled_slope_arrow(x_start = 0, y_start = a,
                                    x_end = 1, y_end = a + b1,
                                    x_label = 3, y_label = 28,
                                    label = "Slope", color = "green")
      
  }
  p + ggtitle(sprintf("%0.2f + %0.2f*x", a, b1))
})
```


In order to account for omitted variables (or possibly true randomness in the data), we need to modify the linear model equation to the form below. 

<!-- Improve the phrasing below. -->
All *constant effects* of the omitted (or even unknown variables) are absorbed into the intercept. (Importantly, the same would happen if those variables were not omitted, but their predictors were centered.)
<!-- There is a lot to expand upon in that above statment. -->

Meanwhile all the *variable effects* are absorbed into the error term $\epsilon$, which is assumed ot be a **normally distributed** **random variable** with mean 0, and some unknown standard deviation $\sigma$; $\epsilon \sim  N(0,\sigma)$

$$ \underbrace{y_i}_{\text{Observed value}} =
            \overbrace{\underbrace{\alpha}_{\text{Intercept}}}^{\text{additive term}} + 
            \overbrace{\underbrace{\beta_1}_{\text{Slope}} * \underbrace{{x_1}_i}_{Predictor}}^{\text{additive term}} + 
            \overbrace{\underbrace{\beta_2}_{\text{Slope}} * \underbrace{{x_2}_i}_{Predictor}}^{\text{additive term}} +  
            \ldots +
            \underbrace{\epsilon_i}_{Error}$$
            
            
```{r, fig.height=2, fig.width=2, fig.align='right'}
inputPanel(
  sliderInput("intercept3", "Intercept", min = -25, max = -15, value = -18, round = FALSE),
  sliderInput("slope3", "Slope", min = 2, max = 6, value = 4, round = FALSE),
  checkboxInput("reset3", "Reset", value = FALSE)
)
```

```{r, fig.height=1.5, fig.width=2, fig.align='right'}
errors <- data.frame()

for (intercept in seq(20, 30, .5)) {
  for(slope in seq(0, 5, .1)) {
    cab_fares2$predicted <- with(cab_fares2, {intercept + slope * distance_km}) 
    cab_fares2$residuals <- with(cab_fares2, {fare_mnt - predicted}) 

    errors <- rbind(errors, data.frame(intercept = intercept, slope = slope, avg_err = mean(abs(cab_fares2$residuals)) )) %>% unique()

  }
}



library(gridExtra)
renderPlot({
  
  if (input$reset3) {
      errors <<- data.frame()
  }

  cab_fares2$predicted <- with(cab_fares2, {input$intercept3 + input$slope3 * distance_km}) 
  cab_fares2$residuals <- with(cab_fares2, {fare_mnt - predicted}) 
  errors <<- rbind(errors, data.frame(intercept = input$intercept3, slope = input$slope3, avg_err = mean(abs(cab_fares2$residuals)) )) %>% unique()
  
  p <- cab_fares2 %>% ggplot(aes(distance_km, fare_mnt)) + geom_point()
  p <- p + geom_abline(intercept = input$intercept3, slope = input$slope3, color = "red")
  p + geom_errorbar(aes(ymin = ifelse(residuals > 0, dist, predicted),
                        ymax = ifelse(residuals > 0, predicted, dist) ), width = 0 )
  
  
  p2 <- errors %>% ggplot(aes(intercept, slope, color = avg_err)) + geom_point(size=10, alpha = 0.5) + theme_bw()

  gridExtra::grid.arrange(p, p2)
  
})
```
